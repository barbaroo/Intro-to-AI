{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Word Embeddings with Word2Vec\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Word embeddings, which are dense vector representations of words, have revolutionized the way we handle textual data in machine learning. Among the various methods to obtain word embeddings, Google's Word2Vec has stood out due to its ability to capture semantic relationships between words. In this notebook, we'll delve into the Word2Vec embeddings and visualize them to understand their structure better.\n",
        "\n",
        "### Objectives:\n",
        "\n",
        "1. **Load a Pretrained Word2Vec Model**: We'll use the `gensim` library to load Google's pretrained Word2Vec model. This model has been trained on a massive amount of data and can capture intricate semantic relationships.\n",
        "\n",
        "2. **Explore the Capabilities of Word2Vec**: We'll showcase some fundamental operations that can be performed using Word2Vec, such as:\n",
        "    - Finding words most similar to a given word.\n",
        "    - Computing the similarity score between two words.\n",
        "    - Solving analogies. For instance, given \"man is to king as woman is to ?\", the model can predict the word that best completes the analogy.\n",
        "\n",
        "3. **Visualize Word Embeddings**: The embeddings are typically in a high-dimensional space (e.g., 300 dimensions). We'll use dimensionality reduction techniques, like PCA, to visualize these embeddings in a 2D space. This visualization will help us understand how similar words cluster together.\n",
        "\n",
        "## Let's Dive In!\n",
        "\n",
        "With the background set, let's dive into the exploration and see the magic of Word2Vec in action!\n"
      ],
      "metadata": {
        "id": "MKMzS-HEwsRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gensim.downloader utility provides a convenient way to download several pre-trained models:"
      ],
      "metadata": {
        "id": "jLAUJU6jxD49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
        "print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7AGmdl6rPZw",
        "outputId": "6794ed10-ffab-4240-dfd5-2d9b967833f4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Pretrained Word2Vec Model\n",
        "\n",
        "Once you've downloaded the model, you can load it into memory:"
      ],
      "metadata": {
        "id": "-VGueoIJvHMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(path, binary=True)\n"
      ],
      "metadata": {
        "id": "iAn8VYibtIdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the model\n",
        "\n",
        "Try different words as input for your model. What is the size of the word embeddings?"
      ],
      "metadata": {
        "id": "8THMrDMtvNtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(model['word'])"
      ],
      "metadata": {
        "id": "ZZkqwhsQuSmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are a few things the model can do:\n",
        "- Find Most Similar Words:"
      ],
      "metadata": {
        "id": "sPzPF7wQvnc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = model.most_similar('cat', topn=5)\n",
        "print(similar_words)\n"
      ],
      "metadata": {
        "id": "tffJzB20tWyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compute Similarity between Two Words:"
      ],
      "metadata": {
        "id": "2Z14gdcBv6b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = model.similarity('king', 'queen')\n",
        "print(similarity)\n",
        "\n",
        "similarity = model.similarity('king', 'lunch')\n",
        "print(similarity)\n"
      ],
      "metadata": {
        "id": "ztZbBu-Wti3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use the Analogy Feature:\n",
        "\n",
        "Given the analogy \"man is to king as woman is to ?\", the model can find the word that best completes the analogy:"
      ],
      "metadata": {
        "id": "dhIIPeMewNQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analogy_result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(analogy_result)\n"
      ],
      "metadata": {
        "id": "WCkIUqK7tpH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Word Embeddings\n",
        "\n",
        "To visualize word embeddings, we need to reduce their dimensionality to 2D. Let's pick a few words and use PCA to reduce their dimensions. Then, we'll plot them:"
      ],
      "metadata": {
        "id": "vCQPXYvNwSiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Choose words to visualize\n",
        "words = ['king', 'queen', 'man', 'woman', 'prince', 'princess', 'boy', 'girl', 'car', 'bike']\n",
        "\n",
        "# Extract embeddings for these words\n",
        "embeddings = [model[word] for word in words]\n",
        "\n",
        "# Use PCA to reduce dimensions to 2D\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i, word in enumerate(words):\n",
        "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], marker='x', color='red')\n",
        "    plt.text(embeddings_2d[i, 0]+0.02, embeddings_2d[i, 1]+0.02, word, fontsize=12)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Word Embeddings in 2D using PCA')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b5wK6bQut1oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downlaoad a dataset for translation"
      ],
      "metadata": {
        "id": "3icc2B5DLDGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation with Huggingface\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Machine translation the task of converting text from one language to another. In this notebook, we will leverage pre-trained models and datasets from Hugging Face, a platform offering a vast array of NLP resources.\n",
        "\n",
        "We will go through the following:\n",
        "\n",
        "### 1. Loading a Translation Dataset from Hugging Face:\n",
        "Hugging Face's `datasets` library offers a plethora of datasets catering to numerous NLP tasks. We will fetch a translation dataset, which will serve as our ground truth for evaluating translation performance.\n",
        "\n",
        "### 2. Acquiring a Pretrained Translation Model and Tokenizer:\n",
        "The power of neural machine translation often lies in models trained on vast amounts of data. Thankfully, Hugging Face's `transformers` library provides access to several state-of-the-art pre-trained models. We will retrieve a model specifically trained for our language pair of interest, along with its tokenizer, facilitating the conversion of text into a format the model can understand.\n",
        "\n",
        "### 3. Evaluating Machine Translation:\n",
        "Once armed with our model, we will put it to the test! After translating an example our dataset's source sentences, we will evaluate the quality of these translations compared to the reference translations. For this, we'll use the BLEU (Bilingual Evaluation Understudy) score, a widely-accepted metric in the NLP community for assessing translation quality.\n",
        "\n",
        "\n",
        "Let's first install a few useful libraries."
      ],
      "metadata": {
        "id": "avSbRn-ijV1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "hOrPm2RyKypZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the dataset from Hugging Face"
      ],
      "metadata": {
        "id": "KUykYURAkDY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('wmt16', 'de-en')\n"
      ],
      "metadata": {
        "id": "0UJSRW5hLH73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the dataset structure\n",
        "print(dataset)\n",
        "\n",
        "# Viewing an example from the training set\n",
        "print(dataset['train'][0])\n"
      ],
      "metadata": {
        "id": "UxiS4aRZL6Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's pick a sentence from the testing dataset"
      ],
      "metadata": {
        "id": "1qLu94SjkKzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_split = dataset['test']\n",
        "line = test_split['translation'][1]\n",
        "line"
      ],
      "metadata": {
        "id": "u7CWLDDyMJiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a Pretrained MarianMT Model for English-to-German Translation\n",
        "\n",
        "In the provided code snippet, we're leveraging the `transformers` library from Hugging Face to load a specific machine translation model and its associated tokenizer:\n",
        "\n",
        "1. **Import Necessary Modules**:\n",
        "   We start by importing the relevant classes:\n",
        "   - `MarianMTModel`: Represents the actual translation model that can convert sequences from one language to another.\n",
        "   - `MarianTokenizer`: Aids in converting text into a format (tokens) that the model can understand and vice-versa.\n",
        "\n",
        "2. **Specify the Model Name**:\n",
        "   We define the `model_name` as `\"Helsinki-NLP/opus-mt-en-de\"`. This points to a pretrained model on the Hugging Face Model Hub that's optimized for English-to-German translations. The naming convention typically follows the pattern `<organization>/<model-name>`, indicating the group or individual that trained the model and the specific model identifier.\n",
        "\n",
        "3. **Load the Model and Tokenizer**:\n",
        "   Using the `from_pretrained` method, we:\n",
        "   - Load the translation model (`AutoModelForSeq2SeqLM`) using the specified `model_name`.\n",
        "   - Load the associated tokenizer (`AutoTokenizer`) that knows how to tokenize English text for this specific model and convert German tokens back into text.\n",
        "\n",
        "With these steps, we're fully equipped to process English text, feed it into our translation model, and obtain German translations.\n"
      ],
      "metadata": {
        "id": "NDZBaEZ9km-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "vMXE1cfUOSpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(line['de'])"
      ],
      "metadata": {
        "id": "gWAoEPIGPMCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text, model, tokenizer):\n",
        "    # Tokenize the source text\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "    # Perform the translation\n",
        "    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
        "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "# Test the function\n",
        "source_text = line['en']\n",
        "translated = translate(source_text, model, tokenizer)\n",
        "print(translated)\n"
      ],
      "metadata": {
        "id": "95iBAW0TRlmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_text"
      ],
      "metadata": {
        "id": "3aOSVqS5UkfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating translation with the BLEU Score\n",
        "\n",
        "BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate the quality of machine-generated translations in relation to human-provided reference translations. Introduced in a [paper](https://www.aclweb.org/anthology/P02-1040.pdf) by Kishore Papineni and others in 2002, BLEU has since become one of the most widely-used metrics in machine translation evaluation.\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        " **Precision of N-grams**:\n",
        "   - At its core, BLEU considers the precision of n-grams (contiguous sequences of n items from a text) in the machine-generated translation with respect to the reference translation.\n",
        "   - The precision is computed for different n-gram lengths, such as unigrams (1-grams), bigrams (2-grams), trigrams (3-grams), and so on.\n",
        "\n",
        "\n",
        "### Interpreting the Score:\n",
        "\n",
        "- The BLEU score ranges from 0 to 1 (or 0% to 100% when multiplied by 100).\n",
        "- A score of 1 indicates that the machine translation matches the human reference translation perfectly.\n",
        "- Different human translators can produce slightly different translations for the same source text.\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "- It may not always correlate perfectly with human judgment, especially for individual sentences.\n",
        "- BLEU assumes that more matches with the reference translation indicate better quality.\n"
      ],
      "metadata": {
        "id": "sGsYjbyhlWzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If `new_translation` is a single string, you can tokenize it as:\n",
        "new_translation_tokens = translated.split()\n",
        "\n",
        "# Similarly, for reference_texts:\n",
        "reference_texts_tokens = [line['de'].split()]\n"
      ],
      "metadata": {
        "id": "3ObQkqYDVat0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the sentence_bleu score from the nltk library"
      ],
      "metadata": {
        "id": "kGEX9DBNmX3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import nltk"
      ],
      "metadata": {
        "id": "fjLTxNiXX90i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smooth = nltk.translate.bleu_score.SmoothingFunction().method2\n",
        "\n",
        "sentence_bleu(reference_texts_tokens, new_translation_tokens, smoothing_function = smooth)"
      ],
      "metadata": {
        "id": "FJMWVc5WYRWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if you have no time to waste?"
      ],
      "metadata": {
        "id": "mdp2gktgnFMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face Pipelines: Simplifying NLP Tasks\n",
        "\n",
        "Hugging Face's `transformers` library offers a high-level utility called `pipelines` that provides an easy-to-use interface for several common natural language processing (NLP) tasks. Built on top of the vast collection of pretrained models available in the library, the `pipelines` utility abstracts away many of the underlying complexities, making it straightforward for both newcomers and experienced practitioners to leverage state-of-the-art NLP models.\n",
        "\n",
        "### Key Features:\n",
        "\n",
        "1. **Predefined Tasks**:\n",
        "   - `pipelines` supports a variety of tasks out-of-the-box, including text classification, token classification, question answering, text generation, and more.\n",
        "\n",
        "2. **Minimal Code**:\n",
        "   - With just a few lines of code, users can obtain meaningful results without needing to worry about tokenization, model inference, or post-processing.\n",
        "\n",
        "3. **Flexibility**:\n",
        "   - While `pipelines` offers simplicity, it doesn't sacrifice flexibility. Users can easily customize the underlying models, tokenizers, and more.\n",
        "\n",
        "4. **Broad Model Support**:\n",
        "   - Whether you're looking to use BERT for token classification, GPT-2 for text generation, or any other model in the Hugging Face Model Hub, there's likely a pipeline ready for it.\n",
        "\n",
        "\n",
        "The `pipelines` utility in the Hugging Face `transformers` library is a powerful tool that democratizes access to state-of-the-art NLP models. Whether you're building an AI-powered chatbot, a document summarization system, or just exploring the capabilities of modern NLP, `pipelines` can accelerate your implementations.\n",
        "\n"
      ],
      "metadata": {
        "id": "c_bK2eZInI4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the translation pipeline\n",
        "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\", tokenizer=\"Helsinki-NLP/opus-mt-en-de\")\n",
        "\n",
        "# Translate a sentence\n",
        "source_text = \"Hello, how are you?\"\n",
        "translation = translator(source_text)\n",
        "\n",
        "print(translation[0]['translation_text'])\n"
      ],
      "metadata": {
        "id": "l1stXbENnKMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation_en_to_da\", model=\"Helsinki-NLP/opus-mt-en-da\", tokenizer=\"Helsinki-NLP/opus-mt-en-da\")\n",
        "# Translate a sentence\n",
        "source_text = \"Hello, how are you?\"\n",
        "translation = translator(source_text)\n",
        "\n",
        "print(translation[0]['translation_text'])"
      ],
      "metadata": {
        "id": "xFBToo6YnQ8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipelines are available for many different tasks!\n",
        "\n",
        "## Available Tasks in Hugging Face's `pipeline` Utility\n",
        "\n",
        "Hugging Face's `pipeline` utility in the `transformers` library provides a high-level, easy-to-use API for various NLP tasks. As of January 2022, the following tasks are supported:\n",
        "\n",
        "- **Feature Extraction**:\n",
        "  - `feature-extraction`\n",
        "  \n",
        "- **Text Classification**:\n",
        "  - `text-classification`\n",
        "  \n",
        "- **Sentiment Analysis**:\n",
        "  - `sentiment-analysis` (alias for `text-classification`)\n",
        "  \n",
        "- **Token Classification**:\n",
        "  - `token-classification`\n",
        "  \n",
        "- **Named Entity Recognition (NER)**:\n",
        "  - `ner` (alias for `token-classification`)\n",
        "  \n",
        "- **Question Answering**:\n",
        "  - `question-answering`\n",
        "  \n",
        "- **Masked Language Modeling**:\n",
        "  - `fill-mask`\n",
        "  \n",
        "- **Summarization**:\n",
        "  - `summarization`\n",
        "  \n",
        "- **Translation**:\n",
        "  - `translation_xx_to_yy` (where `xx` and `yy` are source and target language codes, respectively)\n",
        "  \n",
        "- **Text-to-Text Generation**:\n",
        "  - `text2text-generation`\n",
        "  \n",
        "- **Text Generation**:\n",
        "  - `text-generation`\n",
        "  \n",
        "- **Zero-Shot Classification**:\n",
        "  - `zero-shot-classification`\n",
        "  \n",
        "- **Conversational Models**:\n",
        "  - `conversational`\n",
        "  \n",
        "For translation tasks, the format `translation_xx_to_yy` allows flexibility in specifying any source (`xx`) and target (`yy`) language combination, provided there's a model that supports that particular pair.\n",
        "\n",
        "To always get the most up-to-date list of supported tasks, refer to the official Hugging Face documentation or inspect the source code of the `pipeline` function in the `transformers` library.\n"
      ],
      "metadata": {
        "id": "U57ihuqznf6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try some of these pipelines!"
      ],
      "metadata": {
        "id": "TuAYhvJbqRH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Analyze the sentiment of a sample sentence\n",
        "result = sentiment_analyzer(\"I love Artificial Intelligence !!\")\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "yincDx5NqTU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the text generation pipeline\n",
        "text_generator = pipeline(\"text-generation\")\n",
        "\n",
        "# Generate text based on a given prompt\n",
        "prompt = \"Once upon a time\"\n",
        "generated_text = text_generator(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
        "\n",
        "print('\\n')\n",
        "print(generated_text[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "9HP7BE-rrXj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the question answering pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\")\n",
        "\n",
        "# Define the context and the question\n",
        "context = \"\"\"\n",
        "The Transformers library provides state-of-the-art machine learning architectures like BERT, GPT-2, and RoBERTa.\n",
        "It is designed by Hugging Face for natural language processing tasks such as text classification, extraction, and translation.\n",
        "\"\"\"\n",
        "question = \"Who designed the Transformers library?\"\n",
        "\n",
        "# Extract the answer from the context\n",
        "answer = qa_pipeline(question=question, context=context)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer['answer']} (with score: {answer['score']:.4f})\")\n"
      ],
      "metadata": {
        "id": "JOPge6RGsgJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\")\n",
        "\n",
        "# Define the text\n",
        "text = \"Hugging Face is a company based in New York City. Its Transformers library is very popular in the NLP community.\"\n",
        "\n",
        "# Recognize named entities in the text\n",
        "entities = ner_pipeline(text)\n",
        "\n",
        "# Display the recognized entities\n",
        "for entity in entities:\n",
        "    word = entity[\"word\"]\n",
        "    label = entity[\"entity\"]\n",
        "    score = entity[\"score\"]\n",
        "    print(f\"Entity: {word}, Label: {label}, Score: {score:.4f}\")\n"
      ],
      "metadata": {
        "id": "exT2TnzWs8OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you are ready to bring NLP into your applications!!"
      ],
      "metadata": {
        "id": "btP3O_OZojZ4"
      }
    }
  ]
}